{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Tuner Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kear Tuner has categories from Basic **RandomSearch** to subclass **HyperModel**. \n",
    "It takes an argument **hp** from which you can sample hyperparameters, such as **hp.Int('units', min_value=32, max_value=512, step=32)** (an integer from a certain range)\n",
    "\n",
    "```python\n",
    "# Define a model\n",
    "def build_model(hp):\n",
    "    ...\n",
    "    model.add(layers.Dense(units=hp.Int('units',\n",
    "                                        min_value=32,\n",
    "                                        max_value=512,\n",
    "                                        step=32),\n",
    "                           activation='relu'))\n",
    "    ...\n",
    "    ...\n",
    "\n",
    "\n",
    "# Use random search\n",
    "from kerastuner.tuners import RandomSearch\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=5,\n",
    "    executions_per_trial=3,\n",
    "    directory='my_dir',\n",
    "    project_name='helloworld')\n",
    "\n",
    "# print a summary of the search space\n",
    "tuner.search_space_summary()\n",
    "\n",
    "# start to search the best set of parameters\n",
    "tuner.search(x, y,\n",
    "             epochs=5,\n",
    "             validation_data=(val_x, val_y))\n",
    "\n",
    "# retrieve the best model\n",
    "models = tuner.get_best_models(num_models=2)\n",
    "\n",
    "# print a summary of the results\n",
    "tuner.results_summary()\n",
    "```\n",
    "\n",
    "Use a HyperModel subclass instead of a model-building function. This makes it easy to share and reuse hypermodels.\n",
    "\n",
    "\n",
    "```python\n",
    "from kerastuner import HyperModel\n",
    "\n",
    "class MyHyperModel(HyperModel):\n",
    "    def __init__(self, num_classes):\n",
    "        \n",
    "    def build(self, hp):\n",
    "        ...\n",
    "        \n",
    "    \n",
    "hypermodel = MyHyperModel(num_classes=10)\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    hypermodel,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    directory='my_dir',\n",
    "    project_name='helloworld')\n",
    "\n",
    "tuner.search(x, y,\n",
    "             epochs=5,\n",
    "             validation_data=(val_x, val_y))\n",
    "\n",
    "```\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras Tuner includes pre-made tunable applications: HyperResNet and HyperXception and You can easily restrict the search space to just a few parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Tuner Mnist Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy\n",
    "\n",
    "from kerastuner.tuners import RandomSearch\n",
    "from kerastuner.engine.hypermodel import HyperModel\n",
    "from kerastuner.engine.hyperparameters import HyperParameters\n",
    "\n",
    "\n",
    "(x, y), (val_x, val_y) = keras.datasets.mnist.load_data()\n",
    "x = x.astype('float32') / 255.\n",
    "val_x = val_x.astype('float32') / 255.\n",
    "\n",
    "x = x[:10000]\n",
    "y = y[:10000]\n",
    "\n",
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Flatten(input_shape=(28, 28)))\n",
    "    for i in range(hp.Int('num_layers', 2, 20)):\n",
    "        model.add(layers.Dense(units=hp.Int('units_' + str(i), 32, 512, 32),\n",
    "                               activation='relu'))\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(\n",
    "            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=5,\n",
    "    executions_per_trial=3,\n",
    "    directory='test_dir')\n",
    "\n",
    "tuner.search_space_summary()\n",
    "\n",
    "tuner.search(x=x,\n",
    "             y=y,\n",
    "             epochs=3,\n",
    "             validation_data=(val_x, val_y))\n",
    "\n",
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use HyperModel\n",
    "\n",
    "class MyHyperModel(HyperModel):\n",
    "\n",
    "    def __init__(self, img_size, num_classes):\n",
    "        self.img_size = img_size\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def build(self, hp):\n",
    "        model = keras.Sequential()\n",
    "        model.add(layers.Flatten(input_shape=self.img_size))\n",
    "        for i in range(hp.Int('num_layers', 2, 20)):\n",
    "            model.add(layers.Dense(units=hp.Int('units_' + str(i), 32, 512, 32),\n",
    "                                   activation='relu'))\n",
    "        model.add(layers.Dense(self.num_classes, activation='softmax'))\n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(\n",
    "                hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    \n",
    "# Define search parameters\n",
    "hp = HyperParameters()\n",
    "hp.Choice('learning_rate', [1e-1, 1e-3])\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    max_trials=5,\n",
    "    hyperparameters=hp,\n",
    "    tune_new_entries=False,\n",
    "    objective='val_accuracy')\n",
    "\n",
    "tuner.search(x=x,\n",
    "             y=y,\n",
    "             epochs=5,\n",
    "             validation_data=(val_x, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Tuner HyperResNet on CIFAR 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from kerastuner.applications import HyperResNet\n",
    "from kerastuner import RandomSearch\n",
    "\n",
    "# Import the Cifar10 dataset.\n",
    "NUM_CLASSES = 10\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "y_train = to_categorical(y_train, NUM_CLASSES)\n",
    "y_test = to_categorical(y_test, NUM_CLASSES)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import an hypertunable version of Resnet.\n",
    "hypermodel = HyperResNet(\n",
    "    input_shape=x_train.shape[1:],\n",
    "    classes=NUM_CLASSES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the hypertuner: we should find the model that maximixes the\n",
    "# validation accuracy, using 40 trials in total.\n",
    "tuner = RandomSearch(\n",
    "    hypermodel,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=40,\n",
    "    project_name='cifar10_resnet',\n",
    "    directory='test_directory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display search overview.\n",
    "tuner.search_space_summary()\n",
    "\n",
    "# Performs the hypertuning.\n",
    "tuner.search(x_train, y_train, epochs=10, validation_split=0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the best models, their hyperparameters, and the resulting metrics.\n",
    "tuner.results_summary()\n",
    "\n",
    "# Retrieve the best model.\n",
    "best_model = tuner.get_best_models(num_models=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best model.\n",
    "loss, accuracy = best_model.evaluate(x_test, y_test)\n",
    "print('loss:', loss)\n",
    "print('accuracy:', accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
